{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69a5478-64fb-45de-9e19-c3dd833a8c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/sunnynuri12@gmail.com/Big_project_01/Data_Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428a2281-5afb-445e-8109-1c55a6b2804b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##calling Data Loader\n",
    "dl=DataLoader(\"/FileStore/Big_Proj_01/Bronze_Layer/Insurance_Company.csv\")\n",
    "return_df=dl.create_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bebaa3e6-4be1-41e0-bc69-7767871c7739",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "\n",
    "## getting a df from loader and cleaning the data.\n",
    "class DataCleaning():\n",
    "\n",
    "    ## init method\n",
    "    def __init__(self,df:'Dataframe'):\n",
    "        self.df=df\n",
    "\n",
    "    def clean_data(self):\n",
    "\n",
    "        ## Standardizing dates\n",
    "        standard_dates=return_df.withColumn(\"Opened\",to_date(\"Opened\",\"MM/dd/yyyy\")).withColumn(\"Closed\",to_date(\"Closed\",\"MM/dd/yyyy\"))\n",
    "\n",
    "        ## marking not available in all string columns where null is present\n",
    "        cleaned_df=standard_dates.fillna({\"Coverage\":\"Not Available\",\n",
    "                                \"SubCoverage\":\"Not Available\",\n",
    "                                \"Reason\":\"Not Available\",\n",
    "                                \"SubReason\":\"Not Available\",\n",
    "                                \"Disposition\":\"Not Available\",\n",
    "                                \"Conclusion\":\"Not Available\",\n",
    "                                \"Status\":\"Not Available\",\n",
    "                                \"Recovery\":0})\n",
    "        return cleaned_df\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68649137-faf1-4f80-9357-856f5a975414",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, max, monotonically_increasing_id\n",
    "\n",
    "class DimTables():\n",
    "\n",
    "    def __init__(self,df:\"Dataframe\"):\n",
    "        self.df=df\n",
    "        # Create an instance of DataCleaning inside DimTables\n",
    "        self.cleaned_df = DataCleaning(self.df).clean_data()\n",
    "\n",
    "    def create_dim_tables(self,schema,column_name,table_name):\n",
    "\n",
    "        # Extract unique companies from the new data (only the Company column)\n",
    "        distinct_df = self.cleaned_df.select(column_name).distinct()\n",
    "\n",
    "        # distinct_df.show()\n",
    "\n",
    "        # Try to load the existing Company Table\n",
    "        try:\n",
    "            existing_data_df = spark.read.format(\"delta\").load(f\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/{table_name}\")\n",
    "        except Exception as e:\n",
    "            # If the table doesn't exist, create an empty DataFrame with the correct schema\n",
    "            existing_data_df = spark.createDataFrame([], schema)\n",
    "\n",
    "        # Identify companies that are new (not present in the existing Company Table)\n",
    "        new_data_only = distinct_df.join(existing_data_df, column_name, \"leftanti\")\n",
    "\n",
    "        # # Find the current maximum CompanyID in the existing table\n",
    "        if not existing_data_df.rdd.isEmpty():\n",
    "            max_id = existing_data_df.agg(max(f\"{table_name}ID\")).collect()[0][0]\n",
    "        else:\n",
    "            max_id = 0  # If the table is empty, start from 0\n",
    "\n",
    "        # # Assign new CompanyID to new companies (starting from the next available ID)\n",
    "        # # Cast monotonically_increasing_id to IntegerType and add max_id to it\n",
    "        new_data_with_id = new_data_only.withColumn(f\"{table_name}ID\", (monotonically_increasing_id() + max_id + 1).cast(IntegerType()))\n",
    "\n",
    "\n",
    "        # # Combine existing and new companies, ensuring no duplicate CompanyID\n",
    "        # # Enforce the correct schema for the union\n",
    "        combined_data_df = existing_data_df.unionByName(new_data_with_id.select(f\"{column_name}ID\", column_name))\n",
    "\n",
    "        # #combined_company_df.display()\n",
    "        # # creating delta tables\n",
    "        new_data_with_id.write.format(\"delta\").mode(\"append\").save(f\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/{table_name}\")\n",
    "\n",
    "\n",
    "    def create_all_dims(self):\n",
    "        # List of dimension configurations (column_name, table_name, schema)\n",
    "        dim_configurations = [\n",
    "            (\"Company\", \"company\", StructType([StructField(\"CompanyID\", IntegerType(), nullable=False), StructField(\"Company\", StringType(), nullable=False)])),\n",
    "\n",
    "            (\"Reason\", \"reason\", StructType([StructField(\"ReasonID\", IntegerType(), nullable=False), StructField(\"Reason\", StringType(), nullable=False)])),\n",
    "\n",
    "            (\"SubReason\", \"subreason\", StructType([StructField(\"SubReasonID\", IntegerType(), nullable=False), StructField(\"SubReason\", StringType(), nullable=False)])),\n",
    "\n",
    "            (\"Coverage\", \"coverage\", StructType([StructField(\"CoverageID\", IntegerType(), nullable=False), StructField(\"Coverage\", StringType(), nullable=False)])),\n",
    "\n",
    "            (\"SubCoverage\", \"subcoverage\", StructType([StructField(\"SubCoverageID\", IntegerType(), nullable=False), StructField(\"SubCoverage\", StringType(), nullable=False)])),\n",
    "\n",
    "            (\"Disposition\", \"disposition\", StructType([StructField(\"DispositionID\", IntegerType(), nullable=False), StructField(\"Disposition\", StringType(), nullable=False)])),\n",
    "\n",
    "            (\"Conclusion\", \"conclusion\", StructType([StructField(\"ConclusionID\", IntegerType(), nullable=False), StructField(\"Conclusion\", StringType(), nullable=False)])),\n",
    "\n",
    "            (\"Status\", \"status\", StructType([StructField(\"StatusID\", IntegerType(), nullable=False), StructField(\"Status\", StringType(), nullable=False)]))\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        # Loop through each dimension configuration and create dimension tables\n",
    "        for column_name, table_name, schema in dim_configurations:\n",
    "            self.create_dim_tables(column_name=column_name, table_name=table_name, schema=schema)\n",
    "            # print(column_name, table_name, schema)\n",
    "\n",
    "obj1=DimTables(return_df)\n",
    "dim_tables=obj1.create_all_dims()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e3f6644-f343-4fca-bb78-3f1b06b1d16e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FactTable():\n",
    "\n",
    "    ##define init\n",
    "    def __init__(self, df:'Dataframe'):\n",
    "        self.df=df\n",
    "        self.cleaned_df = DataCleaning(self.df).clean_data()\n",
    "\n",
    "    def load_fact(self):\n",
    "        company_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/company/\")\n",
    "\n",
    "        conclusion_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/conclusion/\")\n",
    "\n",
    "        coverage_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/coverage/\")\n",
    "\n",
    "        disposition_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/disposition/\")\n",
    "\n",
    "        reason_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/reason/\")\n",
    "\n",
    "        status_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/status/\")\n",
    "\n",
    "        subcoverage_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/subcoverage/\")\n",
    "\n",
    "        subreason_df = spark.read.format(\"delta\").load(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/subreason/\")\n",
    "                                                     \n",
    "        fact_df=(\n",
    "            self.cleaned_df\n",
    "            .join(company_df, self.cleaned_df[\"Company\"]==company_df[\"Company\"], \"left\")\n",
    "            .join(conclusion_df, self.cleaned_df[\"Conclusion\"]==conclusion_df[\"Conclusion\"], \"left\") \n",
    "            .join(coverage_df, self.cleaned_df[\"coverage\"]==coverage_df[\"coverage\"], \"left\")\n",
    "            .join(disposition_df, self.cleaned_df[\"disposition\"]==disposition_df[\"disposition\"], \"left\")\n",
    "            .join(reason_df, self.cleaned_df[\"reason\"]==reason_df[\"reason\"], \"left\")\n",
    "            .join(subcoverage_df, self.cleaned_df[\"subcoverage\"]==subcoverage_df[\"subcoverage\"], \"left\")\n",
    "            .join(subreason_df, self.cleaned_df[\"subreason\"]==subreason_df[\"subreason\"], \"left\")\n",
    "            .join(status_df, self.cleaned_df[\"status\"]==status_df[\"status\"], \"left\")\n",
    "            .select(\n",
    "                company_df[\"CompanyID\"].alias(\"CompanyID\"),\n",
    "                conclusion_df[\"ConclusionID\"].alias(\"ConclusionID\"),\n",
    "                coverage_df[\"CoverageID\"].alias(\"CoverageID\"),\n",
    "                disposition_df[\"DispositionID\"].alias(\"DispositionID\"),\n",
    "                reason_df[\"ReasonID\"].alias(\"ReasonID\"),\n",
    "                subcoverage_df[\"SubcoverageID\"].alias(\"SubcoverageID\"),\n",
    "                subreason_df[\"SubreasonID\"].alias(\"SubreasonID\"),\n",
    "                status_df[\"StatusID\"].alias(\"StatusID\"),\n",
    "                self.cleaned_df[\"FileNo\"].alias(\"FileNo\"),\n",
    "                self.cleaned_df[\"Opened\"].alias(\"OpenedDate\"),\n",
    "                self.cleaned_df[\"Closed\"].alias(\"ClosedDate\"),\n",
    "                self.cleaned_df[\"Recovery\"].alias(\"RecoveryAmount\")\n",
    "                    )\n",
    "        )\n",
    "        fact_df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/Big_Proj_01/Silver_Layer/Delta_Tables/fact\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "943d1564-acb4-44c7-a950-f17a636a8ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2006664191890193,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_transformer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
